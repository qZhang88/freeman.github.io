<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.4.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="介绍序列生成有广泛的应用，如机器翻译、序列标注（实识别等）、文本摘要、问答等等。从机器学习的角度来说，序列生成就是给定一些上下文（context）$x$，然后生成序列 $y$（即 $y_1y_2…y_T$）。针对序列生成任务，使用监督学习时，不管是用CNN、RNN还是Transformer来建模型，最终优化的目标大多是最小化negative log-likelihood，目标函数公式如下：  L_">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习与序列生成">
<meta property="og:url" content="http://freeman.one/2019/04/13/reinforced-generation/index.html">
<meta property="og:site_name" content="FreeMan">
<meta property="og:description" content="介绍序列生成有广泛的应用，如机器翻译、序列标注（实识别等）、文本摘要、问答等等。从机器学习的角度来说，序列生成就是给定一些上下文（context）$x$，然后生成序列 $y$（即 $y_1y_2…y_T$）。针对序列生成任务，使用监督学习时，不管是用CNN、RNN还是Transformer来建模型，最终优化的目标大多是最小化negative log-likelihood，目标函数公式如下：  L_">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-04-13T15:20:12.397Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="强化学习与序列生成">
<meta name="twitter:description" content="介绍序列生成有广泛的应用，如机器翻译、序列标注（实识别等）、文本摘要、问答等等。从机器学习的角度来说，序列生成就是给定一些上下文（context）$x$，然后生成序列 $y$（即 $y_1y_2…y_T$）。针对序列生成任务，使用监督学习时，不管是用CNN、RNN还是Transformer来建模型，最终优化的目标大多是最小化negative log-likelihood，目标函数公式如下：  L_">






  <link rel="canonical" href="http://freeman.one/2019/04/13/reinforced-generation/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>强化学习与序列生成 | FreeMan</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">FreeMan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://freeman.one/2019/04/13/reinforced-generation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qiao Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FreeMan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">强化学习与序列生成
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-04-13 21:55:12 / Modified: 23:20:12" itemprop="dateCreated datePublished" datetime="2019-04-13T21:55:12+08:00">2019-04-13</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          
             <span id="/2019/04/13/reinforced-generation/" class="leancloud_visitors" data-flag-title="强化学习与序列生成">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Views: </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>序列生成有广泛的应用，如机器翻译、序列标注（实识别等）、文本摘要、问答等等。从机器学习的角度来说，序列生成就是给定一些上下文（context）$x$，然后生成序列 $y$（即 $y_1y_2…y_T$）。针对序列生成任务，使用监督学习时，不管是用CNN、RNN还是Transformer来建模型，最终优化的目标大多是最小化negative log-likelihood，目标函数公式如下：</p>
<script type="math/tex; mode=display">
L_{nll} = -\sum_{t=1}^T \log p(y_t|y_1,...,y_{t-1}, x) \tag{1}\label{eq1}</script><p>当模型训练完后，一般用如下greedy search的方式生成序列 $\hat{y}=[\hat{y}_1,…\hat{y}_T]$，即选每一个时间步骤上选择概率最大的输出，其中$\hat{y}_t$表示第 $t$ 步生成的输出：</p>
<script type="math/tex; mode=display">
\hat{y}_{t} = \underset{y_t}{\operatorname{argmax}}p(\hat{y}_t|\hat{y}_1,...\hat{y}_{t-1}, x)</script><p>对文本生成（如摘要、翻译）任务来说，评估函数一般是ROUGE, BLEU。监督学习很难直接对这类不可微分的评估函数进行学习。这就使得训练和测试的时候产生了偏差。于是强化学习（Reinforcement Learning）的优势就体现出来。强化学习不需要反馈（reward）或者损失是可微分，因而可以使用任何评估函数用来进行优化学习。<br><a id="more"></a></p>
<h3 id="强化学习框架"><a href="#强化学习框架" class="headerlink" title="强化学习框架"></a>强化学习框架</h3><p>就我所知，Ranzatoet al.(2016)的文章<sup><a href="#fn_1" id="reffn_1">1</a></sup>首先使用强化学习框架来训练序列生成，并参考Williams,1992<sup><a href="#fn_2" id="reffn_2">2</a></sup>和Zaremba &amp; Sutskever, 2015<sup><a href="#fn_3" id="reffn_3">3</a></sup>文中的policy gradient方法REINFORCE算法来训练。强化学习框架<sup><a href="#fn_4" id="reffn_4">4</a></sup>的基本要素有agent，state，action，reward，policy。在序列生成任务中，Ranzato将这些要素按如下方式对应：</p>
<ul>
<li>agent: 序列生成模型（RNN等）</li>
<li>action: 在每一个时间步骤选择下一个输出（词或标注等）</li>
<li>policy: agent的参数$\theta$</li>
<li>state: 模型内部状态（如RNN的hidden units）</li>
<li>reward: 可是任何评估函数分值，如ROUGE等，可在每个action后获得反馈，也可在序列生成完后再反馈</li>
</ul>
<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>训练的目标就是找到参数以最大化反馈（reward）$r( y )$。loss定义为负的反馈期望（negative expected reward）：</p>
<script type="math/tex; mode=display">
L_{rl} = - \sum_{y}p(y|x,\theta)r(y) = -\mathbb{E}_{y \sim p(y|x,\theta)}r(y) \tag{2}\label{eq2}</script><p>公式含义为：给定上下文 $x$ ，对所有可能生成的序列 $y$ 的<strong>反馈</strong>计算概率加权和。最大化期望和，也就是更新参数，使得反馈越大的序列，生成出来的概率越大。然而生成的序列 $y$ 的可能组合太多，完全计算所有可能的序列，然后再计算梯度下降更新几乎不可能。在实际中采用近似处理，即根据模型得到的概率函数 $p(y|x,\theta)$ <strong>采样（Sampling）</strong> 一个序列 $y^s$ (即 $y_1^s,…,y_T^s$)。具体就是对每一个time step $t$，根据概率 $p(y_t|y_1,…,y_{t-1},x,\theta)$ 随机选择一个输出 $y_t^s$ 。</p>
<script type="math/tex; mode=display">
L_{rl} \approx -r(y^s)\sum_{t=1}^T \log p(y_t^s|y_1^s,...,y_{t-1}^s, x, \theta) \tag{3}\label{eq3}</script><p>对比公式$\eqref{eq1}$和公式$\eqref{eq3}$，这里有两点区别：</p>
<ol>
<li>通过强化学习把不可微分的评估函数分值变成了权重，然后类似监督学习一样进行梯度更新</li>
<li>一般监督学习训练时都是使用真实（groud truth）的序列 $y$，而强化学习用的采样的序列$y^s$。</li>
</ol>
<h3 id="策略梯度和REINFORCE算法"><a href="#策略梯度和REINFORCE算法" class="headerlink" title="策略梯度和REINFORCE算法"></a>策略梯度和REINFORCE算法</h3><p>为了计算公式$\eqref{eq2}$的梯度，采用REINFORCE算法<sup><a href="#fn_2" id="reffn_2">2</a></sup>,<sup><a href="#fn_4" id="reffn_4">4</a></sup>。带有不可微分的反馈函数的期望梯度，可以按如下方式计算，即对策略的梯度（策略为参数$\theta$）：</p>
<script type="math/tex; mode=display">
\nabla_\theta L_{rl} = -\mathbb{E}_{y \sim p(y|x,\theta)}[r(y) \nabla_\theta \log p(y|x,\theta)]</script><p>像上面提到的一样，对每一个训练样本，根据概率 $p(y_t|y_1,…,y_{t-1}, x, \theta)$用 Monte-Carlo采样一条序列$y^s = (y_1^s, … ,y_T^s)$。通过采样得到 $y^s$ 后，梯度公式就变为：</p>
<script type="math/tex; mode=display">
\nabla_\theta L_{rl} \approx r(y^s) \nabla_\theta \log p(y^s|x,\theta)</script><p>对比公式$\eqref{eq3}$ $L_{rl}$ 和公式$\eqref{eq1}$ $ L_{nll} $ ，可发现公式$\eqref{eq3}$ $L_{rl}$ 相当于在梯度更新时添加了 $r(y^s)$ 权重，若序列分值越大梯度便越大，反之则梯度减弱。在实际训练的时候，因为随机采样的搜索空间太大，直接进行强化学习会导致不稳定和很难收敛。所以通常先用$L_{nll}$进行预训练，然后再使用用 $L_{rl}$ 训练。</p>
<h3 id="REINFORCE-with-a-Baseline"><a href="#REINFORCE-with-a-Baseline" class="headerlink" title="REINFORCE with a Baseline"></a>REINFORCE with a Baseline</h3><p>Ranzato文章<sup><a href="#fn_1" id="reffn_1">1</a></sup>指出REINFORCE算法会先建立一个baseline $\bar{r}_{t}$。引入baseline后目标函数为（非原文中的公式，此处根据理解改写）：</p>
<script type="math/tex; mode=display">
L_{rl} = - \sum_{t=1}^T （r(y^s) - \bar{r}_{t}) \log p(y_t^s|y_1^s,...,y_{t-1}^s, x, \theta) \tag{4} \label{eq4}</script><p>$\bar{r}_t$含义为：如果$r(y^s) &gt; \bar{r}_t$，鼓励选择当前的输出$y_t^s$，梯度方向为增大$y_t^s$的概率，反之则减少其概率。文中 $\bar{r}_t$ 用RNN的隐藏变量$\mathbf{h}_t$作为输入，使用线性回归模型，$\bar{r}_t = w \cdot \mathbf{h}_t + b $，用最小化损失函数$||\bar{r}_t - r(y^s)||^2$进行训练。</p>
<p>Ranzato文章<sup><a href="#fn_1" id="reffn_1">1</a></sup>采用了一种混合策略，先使用$L_{nll}$训练$N^{nll}$轮。然后混合训练$N^{nll+rl}$轮，混合训练时前$(T - \Delta)$个time step用$L_{nll}$，后面$\Delta$个time step用$L_{rl}$学习。接着继续混合训练$N^{nll+rl}$轮，这次time step分别为$(T-2\Delta)$和$2\Delta$。如此往复直到最终完全使用强化学习训练。</p>
<h3 id="Self-critical-Policy-Gradient"><a href="#Self-critical-Policy-Gradient" class="headerlink" title="Self-critical Policy Gradient"></a>Self-critical Policy Gradient</h3><p>与Ranzato et al.(2016)<sup><a href="#fn_1" id="reffn_1">1</a></sup>不同是，Renie et al.(2016)<sup><a href="#fn_5" id="reffn_5">5</a></sup> 首先介绍了用常数$b$作为baseline，表明它不影响梯度的<strong>期望</strong>，但是可以减少梯度估计时的<strong>方差</strong>。然后Renie et al.使用了self-critical policy sequence training (SCST)算法。算法在训练的每一循环产生两个输出序列：</p>
<ol>
<li>$y^s$: 与上面相同，在每一个解码的步骤上根据 $p(y_t^s|y_1^s,…,y_{t-1}^s)$ 进行采样，随机选择 $y_t^s$ </li>
<li>$\hat{y}$: 在每一个解码的步骤上选择 $p(y_t^s|y_1^s,…,y_{t-1}^s)$ 最大概率，即gready search选择 $\hat{y}_t$ </li>
</ol>
<p>得到两个序列后，强化学习的目标函数如下： </p>
<script type="math/tex; mode=display">
L_{rl} = -(r(y^s) - r(\hat{y})) \sum_{t=1}^T \log p(y_t^s|y_1^s,...,y_{t-1}^s, x) \tag{5} \label{eq5}</script><p>公式$\eqref{eq5}$和公式$\eqref{eq3}$对比多了一个基于gready search生成的序列$\hat{y}$的反馈。作为矫正，也就是将$y^s$和$\hat{y}$进行对比，所以叫<strong>self-critical</strong>。梯度下降更新$L_{rl}$ 的含义相当于，若$r(y^s) \gt r(\hat{y})$，采样的结果比gready search结果好，<strong>增大</strong>采样到的 $y^s$ 的概率，相反则减小其概率。而 $| r(y^s) - r(\hat{y}))|$ 值就是一个弹性的学习率权重。</p>
<p>Paulus et al. (2018)<sup><a href="#fn_6" id="reffn_6">6</a></sup>将$L_{rl}$和$L_{nll}$合在一起，来训练生成式摘要抽取:</p>
<script type="math/tex; mode=display">
L_{mixed} = \gamma L_{nll} + (1-\gamma)L_{rl}</script><p>从最终的目标函数来看，强化学习起到作用基本是给目标函数添加了基于ROUGE或BLEU评估函数带来的权值。有意思的是，只用 $L_{rl}$ 训练的结果在ROUGE评分上会比使用混合的高，但是人工评估，混合目标函数训练出的结果可读性和相关性会更好。这一点在下面的抽取式摘要中得到了类似的印证。</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h5 id="coherence-reward"><a href="#coherence-reward" class="headerlink" title="coherence reward"></a>coherence reward</h5><p>Wu &amp; Hu, 2018<sup><a href="#fn_7" id="reffn_7">7</a></sup>文章使用强化学习从文章中抽取句子作为摘要。强化学习的架构基本相同，只是action变成二选一，即通过{0, 1}判断是否抽取当前句子作为摘要。除此之外，文中在使用ROUGE作为最终的反馈，还在每一个time step上引入了一个新的反馈coherence。最终训练时使用的目标函数为（非原文中的公式，而是基于理解和文中的算法步骤改写）：</p>
<script type="math/tex; mode=display">
L_{rl} = - \sum_{t=1}^T  (r(y^s) + \lambda \sum_{i=t}^T{\tilde{r}_i}) \log p(y_t^s|y_1^s,...,y_{t-1}^s, x) \tag{6} \label{eq6}</script><p>公式中的 $\tilde{r}$ 为在每一个action后立刻得到coherence反馈。文中单独训练了一个网络来判断两句话的coherence分值。若当前句子被选取为摘要，则跟上一句选取为摘要的句子计算词、coherence分值，反之分值为0。</p>
<p>持续更新中…..</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><blockquote id="fn_1">
<sup>1</sup>. Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In _ICLR_ 2016.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In _Machine Learning_, 8:229-256, 1992 <a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. Wojciech Zaremba and Ilya Sutskever. Reinforcement Learning Neural Turing Machines - Revised. In _arXiv preprint arXiv:1505.00521_, 2015<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction.  _MIT Press_, 1988<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_5">
<sup>5</sup>. Steven J Renie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In _CVPR_ 2017<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_6">
<sup>6</sup>. Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. In _ICLR_ 2018.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_7">
<sup>7</sup>. Yuxiang Wu and Baotian Hu. Learning to Extract Coherent Summary via Deep Reinforcement Learning. _AAAI_, 2018<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_8">
<sup>8</sup>. Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and Yejin Choi. Deep communicating agents for abstractive summarization. In _Proc. of NAACL_, 2018<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_9">
<sup>9</sup>. Yaser Keneshloo, Tian Shi, Naren Ramakrishnan and Chandan K. Reddy. Deep Reinforcement Learning For Sequence to Sequence Models, In _arXiv preprint arXiv:1805.09461_, 2018<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a>
</blockquote>
<h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3><p>Ranzato et al.(2016)<sup><a href="#fn_1" id="reffn_1">1</a></sup>文中提到了$\frac{\partial L}{\partial o_t} = softmax(o_t) - \mathbb{1}(y_t)$，其中$o_t = [o_{t1},…,o_{tT}]$ 是每一步softemax的输入，$L$为目标函数。下面对这个梯度公式做出了推导：</p>
<script type="math/tex; mode=display">
\begin{align}
L & = -\sum_{t=1}^T \log p(y_t|y_1,...,y_{t-1}, x) \\
 & = -\sum_{t=1}^T \log(softmax(o_t) \mathbb{1}(y_t)) \\ 
\end{align}</script><p>$\mathbb{1}(i)$是一个指示器(indicator)，只有第$i$元素为1，其余为0。$L$对$o_t$的梯度$\frac{\partial L}{\partial o_t}$推导如下</p>
<ol>
<li>$o_{ti}=y_t$时，对$o_{ti}$的偏导为：</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial L}{\partial o_{ti}} & = -\frac{\partial \log(\frac{e^{o_{ti}}}{\sum_{j=1}^T e^{o_{tj}}})}{\partial o_{ti}} \\
 & = -\frac{\partial ( o_{ti} - \log({\sum_{j=1}^T e^{o_{tj}}}))}{\partial o_{ti}} \\
 & = - 1 + \frac{1}{\sum_{j=1}^T e^{o_{tj}}}e^{o_{ti}}
\end{align}</script><ol>
<li>$o_{tj} \neq y_t$时，对$o_{ti}$的偏导为：</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial L}{\partial o_{tj}} & = -\frac{\partial \log(\frac{e^{o_{ti}}}{\sum_{k=1}^T e^{o_{tk}}})}{\partial o_{tj}} \\
 & = -\frac{\partial ( o_{ti} - \log({\sum_{k=1}^T e^{o_{tk}}}))}{\partial o_{tj}} \\
 & = \frac{1}{\sum_{k=1}^T e^{o_{tj}}}e^{o_{tj}}
\end{align}</script><p>所以最终：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial o_t} = softmax(o_t) - \mathbb{1}(y_t)</script>
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/08/ssh-tunnel/" rel="next" title="autossh内网穿透">
                <i class="fa fa-chevron-left"></i> autossh内网穿透
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qiao Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍"><span class="nav-number">1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#强化学习框架"><span class="nav-number">2.</span> <span class="nav-text">强化学习框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#目标函数"><span class="nav-number">3.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#策略梯度和REINFORCE算法"><span class="nav-number">4.</span> <span class="nav-text">策略梯度和REINFORCE算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#REINFORCE-with-a-Baseline"><span class="nav-number">5.</span> <span class="nav-text">REINFORCE with a Baseline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-critical-Policy-Gradient"><span class="nav-number">6.</span> <span class="nav-text">Self-critical Policy Gradient</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他"><span class="nav-number">7.</span> <span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#coherence-reward"><span class="nav-number">7.0.1.</span> <span class="nav-text">coherence reward</span></a></li></ol></li></ol><li class="nav-item nav-level-3"><a class="nav-link" href="#References"><span class="nav-number">8.</span> <span class="nav-text">References</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Appendix"><span class="nav-number">9.</span> <span class="nav-text">Appendix</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qiao Zhang</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Mist</a> v6.4.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script>



  



  










  





  

  
  <script>
    
    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(counter.time + 1);
            })
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text('Counter not initialized! See more at console err msg.');
              console.error('ATTENTION! LeanCloud counter has security bug, see here how to solve it: https://github.com/theme-next/hexo-leancloud-counter-security. \n But you also can use LeanCloud without security, by set \'security\' option to \'false\'.');
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "guwnmVyWQ32iHmMCUE9Hssrd-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "guwnmVyWQ32iHmMCUE9Hssrd-gzGzoHsz",
                'X-LC-Key': "5J1BwkvD5N4NiK1KDGgpd4a5",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });
  </script>



  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  

</body>
</html>
